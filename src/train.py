# -*- coding: utf-8 -*-
"""audio-thing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/audio-thing-64c50bba-bf5a-410b-aa7d-740cec6197dd.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250124/auto/storage/goog4_request%26X-Goog-Date%3D20250124T083305Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4cbee5989a69c4fb658d8381ed0424ae560638b51398531aa1c7ed62e963683d8ee33f33eb72a9dd24cfdc2dbb9aeb2e4e0302b4a8131e874e0c866e2fea2fd6cc067038b142f56594f2ec769cec323eaaadee04ea0a190abb497f3029d5a259a9a2aa2236ab53700d40c5caf632c0b542e096d365c7fd72163bb5d0c5db59b8d2ce2265d68a1b49c49591d3cf1997cd99d436f300742f14a4ccb858bb51215b64e210d44388bef58d4f6e3c79f0c67e59b788b7a9fe1fe3e31028f3701ed63143710317391f6c92f63633866eaed93e5a2cd04e68f50c0910fab7aff291eca18f383ad5c813ad0c384af0312a8f25b669d317ae33b207c192768b3b866598d7
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
organizations_mozillaorg_common_voice_path = kagglehub.dataset_download('organizations/mozillaorg/common-voice')

print('Data source import complete.')

# Commented out IPython magic to ensure Python compatibility.
# %%writefile config.yaml
# experiments:
#   - dataset_size: 0.1
#     learning_rate: 1e-3
#     epochs: 5
#   - dataset_size: 0.3
#     learning_rate: 1e-4
#     epochs: 10
#   - dataset_size: 0.5
#     learning_rate: 5e-5
#     epochs: 15
#   - dataset_size: 1.0
#     learning_rate: 1e-5
#     epochs: 20

# %% [code] -- Imports
import torch
from torch import nn
from torch.cuda.amp import GradScaler, autocast  # For mixed precision training
from torch.utils.data import DataLoader, Dataset
from transformers import Wav2Vec2Processor, Wav2Vec2Model
from tqdm import tqdm  # For progress bars
import pandas as pd
import librosa
import os
import time
import glob

# Configuration
CONFIG = {
    "dataset" :{
        "root": "/kaggle/input/common-voice",
    },
    "audio": {
        "sr": 16000,               # Sample rate
        "clip_duration": 1.0,      # Duration of each clip in seconds
        "max_clips": 200,          # Maximum number of clips per audio file
        "min_duration": 1.0,
    },
    "model": {
        "embed_dim": 768,          # Dimension of pretrained embeddings
        "transformer_layers": 4,
        "nhead": 8,
        "dim_feedforward": 2048,
        "num_classes": 10,         # Update based on your dataset
        "dropout": 0.1
    },
    "training": {
        "batch_size": 32,
        "lr": 1e-4,
        "epochs": 20,
        "device": "cuda" if torch.cuda.is_available() else "cpu"
    },
    "output": {
        "model_dir": "/kaggle/working/models"
    },
}

# %% [code] -- Pretrained Embedding Model
class AudioEmbedder:
    def __init__(self):
        self.processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
        self.model = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base-960h", ignore_mismatched_sizes=True)
        self.model.eval().to(CONFIG["training"]["device"])
        for param in self.model.parameters():
            param.requires_grad = False

    def get_embeddings(self, audio):
        with torch.no_grad():
            inputs = self.processor(audio, sampling_rate=CONFIG["audio"]["sr"],
                                   return_tensors="pt", padding=True).to(CONFIG["training"]["device"])
            outputs = self.model(**inputs).last_hidden_state
            return outputs.mean(dim=1)  # Average over time dimension

# %% [code] -- Data Class
class AudioClipDataset(Dataset):
    def __init__(self, file_list, labels):
        self.file_list = file_list
        self.labels = labels
        self.embedder = AudioEmbedder()
        self.clip_length = int(CONFIG["audio"]["sr"] * CONFIG["audio"]["clip_duration"])

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        # Load audio
        audio, _ = librosa.load(self.file_list[idx], sr=CONFIG["audio"]["sr"])

        # Pad audio to minimum 1 second if needed
        if len(audio) < self.clip_length:
            audio = librosa.util.fix_length(audio, size=self.clip_length)

        # Split into 1-second clips
        clips = librosa.util.frame(
            audio,
            frame_length=self.clip_length,
            hop_length=self.clip_length
        ).T

        # Generate embeddings
        clip_embeddings = []
        for clip in clips:
            # Pad individual clips (in case of rounding errors)
            if len(clip) < self.clip_length:
                clip = librosa.util.fix_length(clip, size=self.clip_length)

            embeddings = self.embedder.get_embeddings(clip)
            clip_embeddings.append(embeddings.cpu())

        # Pad/Truncate to max_clips
        embeddings_tensor = torch.cat(clip_embeddings, dim=0)
        if embeddings_tensor.shape[0] > CONFIG["audio"]["max_clips"]:
            embeddings_tensor = embeddings_tensor[:CONFIG["audio"]["max_clips"]]
        else:
            padding = torch.zeros((
                CONFIG["audio"]["max_clips"] - embeddings_tensor.shape[0],
                CONFIG["model"]["embed_dim"]
            ))
            embeddings_tensor = torch.cat([embeddings_tensor, padding], dim=0)

        return embeddings_tensor, torch.tensor(self.labels[idx])

# %% [code] -- Transformer Model
class ClipTransformer(nn.Module):
    def __init__(self, num_classes):  # Now accepts num_classes as parameter
        super().__init__()
        self.pos_encoder = nn.Embedding(CONFIG["audio"]["max_clips"], CONFIG["model"]["embed_dim"])
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=CONFIG["model"]["embed_dim"],
            nhead=CONFIG["model"]["nhead"],
            dim_feedforward=CONFIG["model"]["dim_feedforward"],
            dropout=CONFIG["model"]["dropout"],
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, CONFIG["model"]["transformer_layers"])
        self.classifier = nn.Linear(CONFIG["model"]["embed_dim"], num_classes)  # Use parameter

    def forward(self, x):
        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)
        x = x + self.pos_encoder(positions)
        x = self.transformer(x)
        x = x.mean(dim=1)
        return self.classifier(x)

import glob
import torch
import torch.nn as nn
import torch.multiprocessing as mp
from torch.utils.data import DataLoader
from tqdm import tqdm
from torch.cuda.amp import autocast
import time
import os
import pandas as pd

def train():
    # Set multiprocessing start method for CUDA compatibility
    mp.set_start_method('spawn', force=True)

    # Load metadata
    train_df = pd.read_csv(os.path.join(CONFIG["dataset"]["root"], "cv-valid-train.csv"))
    valid_df = pd.read_csv(os.path.join(CONFIG["dataset"]["root"], "cv-valid-dev.csv"))

    # Filter valid samples
    train_df = train_df[
        (train_df['accent'].notna()) &
        (train_df['up_votes'] > train_df['down_votes'])
    ]
    valid_df = valid_df[
        (valid_df['accent'].notna()) &
        (valid_df['up_votes'] > valid_df['down_votes'])
    ]

    # Setup checkpoint directory
    checkpoint_dir = os.path.join(CONFIG["output"]["model_dir"], "checkpoints")
    os.makedirs(checkpoint_dir, exist_ok=True)
    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, "checkpoint_*.pth"))

    # Initialize resume training variables
    resume = False
    start_epoch = 0
    best_acc = 0
    label_map = None

    # Check for existing checkpoints
    if checkpoint_files:
        epochs = [int(os.path.splitext(os.path.basename(f).split('_')[-1])[0]) for f in checkpoint_files]
        latest_epoch = max(epochs)
        latest_checkpoint = os.path.join(checkpoint_dir, f"checkpoint_{latest_epoch}.pth")
        print(f"Found existing checkpoint at epoch {latest_epoch+1}")

        resume = input("Resume training from checkpoint? (y/n): ").lower() == 'y'

        if resume:
            checkpoint = torch.load(latest_checkpoint)
            label_map = checkpoint['label_map']
            start_epoch = checkpoint['epoch'] + 1
            best_acc = checkpoint['best_acc']

    # Create or load label mapping
    if not resume:
        all_accents = pd.concat([train_df['accent'], valid_df['accent']]).unique()
        label_map = {accent: idx for idx, accent in enumerate(sorted(all_accents))}

    num_classes = len(label_map)

    # Create datasets (using original AudioClipDataset without modifications)
    train_dataset = AudioClipDataset(
        file_list=[os.path.join(CONFIG["dataset"]["root"], "cv-valid-train", f) for f in train_df['filename']],
        labels=train_df['accent'].map(label_map).values
    )

    valid_dataset = AudioClipDataset(
        file_list=[os.path.join(CONFIG["dataset"]["root"], "cv-valid-dev", f) for f in valid_df['filename']],
        labels=valid_df['accent'].map(label_map).values
    )

    # Create DataLoaders with safe parameters
    train_loader = DataLoader(
        train_dataset,
        batch_size=CONFIG["training"]["batch_size"],
        shuffle=True,
        num_workers=0,  # Start with 0 workers for stability
        pin_memory=True
    )

    valid_loader = DataLoader(
        valid_dataset,
        batch_size=CONFIG["training"]["batch_size"],
        num_workers=0,
        pin_memory=True
    )

    # Initialize model with DataParallel
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    model = ClipTransformer(num_classes=num_classes)

    # Handle multi-GPU setup
    if torch.cuda.device_count() > 1:
        print(f"Using {torch.cuda.device_count()} GPUs!")
        model = nn.DataParallel(model)
    model = model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG["training"]["lr"])
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3)
    scaler = torch.cuda.amp.GradScaler(enabled=True)

    # Load checkpoint state if resuming
    if resume:
        # Handle DataParallel wrapping
        try:
            model.load_state_dict(checkpoint['model'])
        except RuntimeError:
            # If loading to DataParallel wrapped model
            model.module.load_state_dict(checkpoint['model'])

        optimizer.load_state_dict(checkpoint['optimizer'])
        scheduler.load_state_dict(checkpoint['scheduler'])
        scaler.load_state_dict(checkpoint['scaler'])
        print(f"Resuming training from epoch {start_epoch+1}")

    # Training loop
    for epoch in range(start_epoch, CONFIG["training"]["epochs"]):
        model.train()
        train_loss = 0
        start_time = time.time()

        for inputs, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}"):
            # Move data to device
            inputs = inputs.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()

            with autocast():
                outputs = model(inputs)
                loss = criterion(outputs, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            train_loss += loss.item()

        # Validation phase
        model.eval()
        val_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, labels in valid_loader:
                inputs = inputs.to(device)
                labels = labels.to(device)

                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item()

                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        # Update learning rate and metrics
        val_acc = correct / total
        scheduler.step(val_acc)

        # Save best model
        if val_acc > best_acc:
            best_acc = val_acc
            # Save using safe method
            save_model = model.module if isinstance(model, nn.DataParallel) else model
            torch.save(save_model.state_dict(),
                      os.path.join(CONFIG["output"]["model_dir"], "best_model.pth"))

        # Save checkpoint
        checkpoint = {
            'epoch': epoch,
            'model': model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(),
            'optimizer': optimizer.state_dict(),
            'scheduler': scheduler.state_dict(),
            'scaler': scaler.state_dict(),
            'best_acc': best_acc,
            'label_map': label_map,
        }
        checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_{epoch}.pth")
        torch.save(checkpoint, checkpoint_path)

        # Print metrics
        epoch_time = time.time() - start_time
        print(f"\nEpoch {epoch+1}/{CONFIG['training']['epochs']}")
        print(f"Train Loss: {train_loss/len(train_loader):.4f} | "
              f"Val Loss: {val_loss/len(valid_loader):.4f} | "
              f"Val Acc: {val_acc:.4f} | "
              f"Time: {epoch_time:.2f}s | "
              f"LR: {optimizer.param_groups[0]['lr']:.2e}")

if __name__ == '__main__':
    train()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# 1. График обучения
def plot_training_curves(train_loss, val_acc):
    plt.figure(figsize=(12, 5))


    # Loss
    plt.subplot(1, 2, 1)
    plt.plot(train_loss, label='Train Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    # Accuracy
    plt.subplot(1, 2, 2)
    plt.plot(val_acc, label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.savefig('training_curves.png')
    plt.show()

# 2. Матрица ошибок
def plot_confusion_matrix(y_true, y_pred, classes):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(15, 10))
    sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.savefig('confusion_matrix.png')
    plt.show()

# 3. Отчёт о классификации
def print_classification_report(y_true, y_pred, classes):
    print(classification_report(y_true, y_pred, target_names=classes))